# 机器学习面试
## 机器学习中,训练步骤?
> 1.读取数据,可以看做数据收集,数据的原始状态,这里是CSV文件,具体读取方式有很多,这里采用pandas的方法
```
def opencsv():  # 使用pandas打开
    data = pd.read_csv('data/train.csv')
    data1 = pd.read_csv('data/test.csv')
    train_x = data.values[0:, 1:]  # 读入全部训练数据
    train_y = data.values[0:, 0]
    result_x = data1.values[0:, 0:]  # 测试全部测试个数据
    return train_x, train_y, result_x
```
> 2.数据预处理,是对数据提前进行处理和修正,主要包括:特征提取,特征降维,特征空值处理,特征转换(one-hot),特征归一化;目标空值处理,目标值转换(one-hot)
(其中常见的是:特征提取,降维.空值,one-hot转换,归一化.)一般使用PCA(主成分分析)

> 3. 交叉验证数据划分,为了模型测试,先选择交叉验证方法,提前划分好数据
```
def data_pro(x,y):
    x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.1,random_state=33)
    return x_train, x_test, y_train, y_test
```


## PCA降维方法

* 为什么要使用降维? 高维数据更加稀疏,一维数据68%会落在正负标准差之间,但是10维数据只有0.02%;稀疏的数据很难描述其特征.
* PCA 最大化各个特征的方差!!!!,使得每个特征都能表示数据的不同特性.
* 对于给定的样本数据,假设其有m维特征,那么对于每一维度特征来说,该维度的方差代表了该维度样本数据与其均值之间的差异,方差越大,差异越大,则该特征可以更好地表达样本数据的特性,因此简单来说,PCA就是选取m维特征中方差较大的特征,而且保证每次选取得特征之间相互独立,得到n维的特征,实现降维的同时使得新的样本特征能够代表数据更重要的信心,这些选出来的差异大而且独立的特征就称之为主成分.
* 选取方差最大的特征方向为第一个坐标轴 => 选取叠一个坐标轴正交且方差次大的方向为第二个坐标轴 =>重复该过程,最后发现n次之后剩下的特征方差几乎为0,则舍去这些维度的特征.
* PCA流程
> 输入样本数据集: D = {x1,x2,x3,...},每一个特征都有1*n维度,我们需要降到m维.

> 1.每个样本中心化,即减去平均值: x1 = x1-x_mean;

> 2.计算协方差矩阵: $M = X^T*X$;

> 3.计算协方差矩阵特征值与特征向量;

> 4.排序特征值,取m个大的特征值对应特征向量,得到特征向量矩阵;

> 5.每个样本: $x_1 =W^T*x_1$,则得到新的m维矩阵向量

```
x = np.random.random(shape=(10,3))



```
## 有监督学习和无监督学习的区别?
## 机器学习中分类的函数?
## l1范数和l2范数
## 归一化操作有哪些,有什么好处?
## 边缘检测算法有哪些?
## 霍夫曼变化的原理?
## C++ 和 C 编译不同的地方?
