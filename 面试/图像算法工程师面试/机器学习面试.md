# 机器学习面试
## 机器学习中,训练步骤?
1.读取数据,可以看做数据收集,数据的原始状态,这里是CSV文件,具体读取方式有很多,这里采用pandas的方法
```
def opencsv():  # 使用pandas打开
    data = pd.read_csv('data/train.csv')
    data1 = pd.read_csv('data/test.csv')
    train_x = data.values[0:, 1:]  # 读入全部训练数据
    train_y = data.values[0:, 0]
    result_x = data1.values[0:, 0:]  # 测试全部测试个数据
    return train_x, train_y, result_x
```
2.数据预处理,是对数据提前进行处理和修正,主要包括:特征提取,特征降维,特征空值处理,特征转换(one-hot),特征归一化;目标空值处理,目标值转换(one-hot)
(其中常见的是:特征提取,降维.空值,one-hot转换,归一化.)一般使用PCA(主成分分析)

3. 交叉验证数据划分,为了模型测试,先选择交叉验证方法,提前划分好数据
```
def data_pro(x,y):
    x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.1,random_state=33)
    return x_train, x_test, y_train, y_test
```

4. 模型建立及测试,用处理好的数据建立训练模型,对模型的评价参数有很多,常用的有:得分(对的比例),查准率,查全率,F1指数

```
    #训练
    knnClf = KNeighborsClassifier()  # k=5   KNN中邻值为5，
    knnClf.fit(x_train, ravel(y_train))

    #预测
    y_predict = knnClf.predict(x_test)
    print("score on the testdata:",knnClf.score(x_test,y_test))
    # print("score on the traindata:",knnClf.score(x_train,y_train))
    print(classification_report(y_test,y_predict))
```
5. 预测的可能性计算,计算分类概率大小
```
   # 可能性
    probablity = knnClf.predict_proba(x_test)
    list_pro = []
    for i in range(probablity.shape[0]):
        pro = max(list(probablity[i]))
        list_pro.append(pro)
```

6、结果保存 将编号，原始结果，预测结果，预测概率保存csv


## PCA降维方法 [参考这篇文章](https://blog.csdn.net/u013166817/article/details/83412067)

* 为什么要使用降维? 高维数据更加稀疏,一维数据68%会落在正负标准差之间,但是10维数据只有0.02%;稀疏的数据很难描述其特征.
* PCA 最大化各个特征的方差!!!!,使得每个特征都能表示数据的不同特性.
* 对于给定的样本数据,假设其有m维特征,那么对于每一维度特征来说,该维度的方差代表了该维度样本数据与其均值之间的差异,方差越大,差异越大,则该特征可以更好地表达样本数据的特性,因此简单来说,PCA就是选取m维特征中方差较大的特征,而且保证每次选取得特征之间相互独立,得到n维的特征,实现降维的同时使得新的样本特征能够代表数据更重要的信心,这些选出来的差异大而且独立的特征就称之为主成分.
* 选取方差最大的特征方向为第一个坐标轴 => 选取叠一个坐标轴正交且方差次大的方向为第二个坐标轴 =>重复该过程,最后发现n次之后剩下的特征方差几乎为0,则舍去这些维度的特征.
* PCA流程
> 输入样本数据集: D = {x1,x2,x3,...},每一个特征都有1*n维度,我们需要降到m维.

> 1.每个样本中心化,即减去平均值: x1 = x1-x_mean;

> 2.计算协方差矩阵: $M = X^T*X$;

> 3.计算协方差矩阵特征值与特征向量;

> 4.排序特征值,取m个大的特征值对应特征向量,得到特征向量矩阵;

> 5.每个样本: $x_1 =W^T*x_1$,则得到新的m维矩阵向量

```
x = np.random.random(shape=(10,3))
```
----
## 有监督学习和无监督学习的区别?
1. 监督学习是从给定的训练集中学习出一个模型,当新的数据到来时,可以根据这个模型预测结果.监督学习的训练集要求包括输入输出,也可以说是特征和目标.训练集中的目标是由人标注的.监督学习最常见的分类(注意和聚类区分)问题,通过已经有的训练样本,去训练得到一个最优模型,在利用这个模型降素有的输入映射到相应的输出,对输出进行简单的判断从而实现分类的目的.常见的有监督学习算法有:回归分析和统计分类.最典型的算法就是KNN和SVM,回归问题就是拟合(x,y)的一条直线,使得代价函数最小.统计分类则是,计算X属于Y的概率的负对数.

2. 无监督学习 输入数据没有标记,也没有确定的结果,样本数据类别未知,需要根据样本间的相似性进行分类(聚类),试图使得类内差距最小化,类间差距最大化.无监督学习的方法分为两类,基于概率密度函数估计的直接方法,指设法找到各类在特征空间的分布参数,在进行分类;另一类称之为样本件相似性度量的简洁聚类方法,其原理是设法定出不同类别的核心或初始内核,然后依据与核心之间的相似性度量将样本聚类成不同的类别.

3. 两者之间的不同点

> &emsp;&emsp; 有监督学习方法必须要有训练集与测试集,在训练集中找到规律.而对测试集使用这种规律,而非监督学习没有训练集,只有一组数据,需要在该数据内寻找规律
.
> &emsp;&emsp;有监督的学习是识别事物,识别的结果表现在给待定数据加上了标签,因此训练样本集必须由带标签的样本组成,而非监督学习方法只要有分析的数据集的本身,预先没有什么标签,所有预测数来的结果也没有标签;

> &emsp;&emsp; 非监督学习方法在寻找数据集中的规律性,这种规律性并不一定能够达到划分数据集的目的,也就是说不一定要分类.
----
## 机器学习中分类的函数?
## l1范数和l2范数 [参考这篇文章](https://blog.csdn.net/program_developer/article/details/79436657)
都是正则化操作,防止过拟合
> 1.什么是范数? 距离的定义是一个宽泛的概率,只要满足非负,自反,三角不等式就可以称之为距离.范数是一种强化的距离概念,它在定义上比距离多了一条数乘的运算法则,可以把范数当做距离来理解
> 2.为什么需要用到范数? 在训练过程中,如果出现过拟合的时候,会导致模型泛化能力下降,正则化是一种回归的形式,他将系数估计朝零的方向进行约束,正则化可以在学习过程中降低模型复杂度和不稳定的程度,避免过拟合.

## 归一化操作有哪些,有什么好处?
## 边缘检测算法有哪些?
## 霍夫曼变化的原理?
## C++ 和 C 编译不同的地方?
